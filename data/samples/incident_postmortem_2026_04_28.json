{
  "meeting_id": "incident_postmortem_2026_04_28",
  "title": "Incident Postmortem - April 27 Outage",
  "date": "2026-04-28",
  "duration_minutes": 45,
  "participants": ["Mike Thompson (CTO)", "Bob Martinez (Eng Lead)", "Carlos Rivera (Infrastructure)", "Eric Zhang (Backend)", "Sarah Chen (VP Product)"],
  "transcript": "Mike: Let's do a blameless postmortem on yesterday's outage. Carlos, walk us through what happened.\n\nCarlos: At 2:47 PM Pacific, our main application became unresponsive. Users saw 502 errors. Total downtime was 43 minutes until full recovery at 3:30 PM.\n\nMike: What was the root cause?\n\nCarlos: The database ran out of connections. We have a connection pool of 100, and a sudden spike in traffic exhausted all available connections. New requests couldn't get database connections and timed out.\n\nMike: What caused the traffic spike?\n\nBob: We traced it back to a marketing email that went out at 2:45 PM. Jennifer's team sent a campaign to our entire user base - about 50,000 users. The click-through rate was higher than expected, around 8%, which translated to 4,000 simultaneous users hitting the site.\n\nSarah: Our normal concurrent users is what, around 500?\n\nCarlos: Correct. We went from 500 to 4,000 in about 3 minutes.\n\nMike: Why didn't our auto-scaling handle this?\n\nCarlos: Auto-scaling kicked in for the application servers. We went from 4 pods to 12 pods. The problem is the database is a single instance - it doesn't auto-scale. More app servers meant more connections to the same database.\n\nEric: Also, some of our queries are pretty heavy. The landing page we linked to runs a dashboard query that takes about 500ms. Under normal load that's fine, but with 4,000 users it created a backlog.\n\nMike: So we have two issues - database connection limits and slow queries. Let's talk mitigations.\n\nCarlos: For immediate fix, I've increased the database connection pool to 200 and upgraded to a larger database instance. That gives us headroom for about 8,000 concurrent users.\n\nMike: What's the cost impact?\n\nCarlos: About $400 per month more for the larger instance. Worth it.\n\nMike: Approved. What about the slow queries?\n\nEric: I've identified the three slowest queries on that dashboard. Two of them can be optimized with better indexes. One of them should probably be moved to a background job with cached results.\n\nBob: How long to implement those fixes?\n\nEric: Indexes I can add today. The background job refactor is about 2 days of work.\n\nMike: Prioritize it. I don't want this to happen again.\n\nSarah: From a process perspective, should we have known about the marketing email?\n\nBob: Good question. We had no idea it was going out.\n\nMike: That's a communication gap. We need a policy - any marketing campaign expected to generate more than 1,000 clicks should be flagged to engineering 48 hours in advance.\n\nSarah: I'll coordinate with Jennifer to set that up.\n\nCarlos: On the monitoring side, we had alerts but they came too late. The connection pool alert triggered at 90% utilization, which was only about a minute before we hit 100%.\n\nMike: What should the threshold be?\n\nCarlos: I'm lowering it to 70%. That gives us about 5 minutes of warning at typical growth rates.\n\nBob: We should also add a dashboard alert. If traffic increases by more than 200% in a 5-minute window, we should get notified.\n\nCarlos: Good idea. I'll implement that today.\n\nMike: Let's talk about the response. How did we find out about the outage?\n\nCarlos: Customer support Slack channel. Users were reporting errors about 5 minutes before our alerts triggered.\n\nMike: That's not great. We should know before our customers.\n\nCarlos: Agreed. The synthetic monitoring we have only checks every 5 minutes. I'm going to reduce that to 1 minute.\n\nMike: Do it. What about the recovery actions?\n\nCarlos: Once we identified the database issue, we killed long-running queries and restarted the application pods. That freed up connections. Then I did the emergency database upgrade, which required about 10 minutes of additional downtime.\n\nMike: Could we have recovered faster?\n\nCarlos: If we had connection pooling at the application level instead of relying on the database, we could have queued requests instead of failing them. That's a bigger architectural change though.\n\nBob: We've talked about adding PgBouncer for connection pooling. This is a good reason to prioritize it.\n\nMike: Add it to the Q3 infrastructure roadmap. Not blocking payments launch but should happen this quarter.\n\nBob: Will do.\n\nMike: Let me summarize the action items. Immediate: increase connection pool to 200, done by Carlos. Today: add database indexes, lower alert threshold, add traffic spike alert. This week: optimize the heavy query with background job. Q3: implement PgBouncer. Process change: marketing notifies engineering 48 hours before large campaigns.\n\nCarlos: One more thing - we should have a documented runbook for this scenario. Currently our incident response was mostly tribal knowledge.\n\nMike: Good point. Carlos, draft a runbook for database connection exhaustion. Include the symptoms, diagnostic steps, and recovery actions.\n\nCarlos: On it.\n\nMike: Final thought - we got lucky this was during business hours when the team was available. If this had happened at 2 AM, it would have been much worse. Carlos, let's discuss on-call rotation improvements separately.\n\nCarlos: Agreed.\n\nMike: Any other concerns?\n\nSarah: Should we communicate anything to customers about what happened?\n\nMike: Our status page showed the outage. For enterprise customers, Grace should proactively reach out with a brief explanation. Generic users probably don't need individual follow-up.\n\nSarah: I'll let Grace know.\n\nMike: Alright, good postmortem everyone. Let's make sure this doesn't happen again."
}
